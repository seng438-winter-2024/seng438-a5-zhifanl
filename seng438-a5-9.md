**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 – Software Reliability Assessment**

| Group \#:       |   |
|-----------------|---|
| Student 1  |  Zhifan Li |
|  Student 2| Sandip Mishra  |
| Student 3  |  Shanzi Ye |
|   Student 4 |  Fardin Aryan |

# Introduction



This assignment provided us with an introduction to the evaluation of integration test data through the use of reliability assessment tools, emphasizing the importance of ensuring system quality and reliability. We implemented two failure data assessment methods:

1. Reliability growth testing
2. Reliability assessment via the Reliability Demonstration Chart (RDC)

These methods are of paramount importance in practical applications as they provide valuable insights into system effectiveness and reliability. This allows for early detection of potential issues by programmers, who can then take appropriate corrective measures. The use of these assessment tools enables stakeholders to make informed decisions about their systems' reliability and implement measures to enhance their performance and lifespan.

The objective of this lab was to gain practical experience in assessing the reliability of a hypothetical system by analyzing its failure data collected during integration testing. To achieve this, the lab was divided into two sections, each focusing on one of the above-mentioned failure data assessment methods.

In the first section of the lab, we were required to establish a reliability growth assessment tool, such as SRTAT or CSFRAT, and generate diagrams depicting the failure rate and reliability of the system under test (SUT). We chose to use SRTAT for our reliability growth testing, gaining familiarity with the features and operation of a reliability growth testing tool.

In the second section, we were introduced to the Reliability Demonstration Chart (RDC) tool and its application. A key goal of this section was to understand how to ascertain the adequacy of testing for a given Mean Time To Failure (MTTF) of the SUT by plotting the test data.

Upon completion of this lab, we have gained a deeper understanding of reliability growth testing and its importance. This lab report details the testing tools used in the experiment, including C-SFRAT or SRTAT, applied to the provided failure data set, which includes various test data files. We selected one file for each of the failure data analysis methods.

# Assessment Using Reliability Growth Testing 
3 Plots for MTTFMin, Twice and Half of It for Your Test Data

In the data drawn from Failure Report 8, we filled out sections of the Reliability-Demonstration-Chart spreadsheet to perform RDC testing. This dataset records 16 defects emerging over several days. We're adjusting the maximum threshold of tolerable defects per quantity of input events to scrutinize its impact on the Mean Time To Failure (MTTF).

This analysis reveals a Fault Incidence Rate (FIR) of 4 defects every 43,200 seconds, equating to 4 defects per 12-hour period.

It is noted that the MTTF, given a defect rate of 4 per 12 hours, is calculated to be 10,800 seconds, or 3 hours. Under these conditions, the graph remains in the “continue testing” zone until the occurrence of the 9th event, at which point it transitions into the “accept” category. Thus, the minimum MTTF is established at 3 hours.

Another observation indicates a FIR of 4 defects over 86,400 seconds, which corresponds to 4 defects every 24 hours.

From this, it is evident that the MTTF at a defect rate of 4 per 24 hours reaches 21,600 seconds, or 6 hours. Given these metrics, the graph initially falls within the “continue testing” area but eventually moves into the “reject” territory. The MTTF of 6 hours represents twice the minimum MTTF. Increasing the MTTF heightens the minimum acceptance criteria, causing the graph to shift toward the “reject” region.

The final assessment shows a FIR of 4 defects every 21,600 seconds, translating to 4 defects every 6 hours.

This finding shows that the MTTF, at a defect frequency of 4 per 6 hours, stands at 5,400 seconds, or 1.5 hours. Under these parameters, the chart begins in the “continue testing” area before rapidly entering the “accept” territory. The MTTF of 1.5 hours is half of the minimum MTTF. Lowering the MTTF reduces the minimum acceptance criteria, moving the chart into the “accept” region.

## Explain Your Evaluation and Justification of How You Decide The MTTFmin

The minimum Mean Time to Failure (MTTFmin) is based on a system's capability of handling certain failures, stress. It demonstrates the reliability of the system. After comparing and analyzing these three graphs, we made a conclusion that MTTFmin is ~2.9 hours. Which is the time when the line starts entering the accepting zone.

## Advantages of RDC

One of the primary advantages of the RDC is its ability to visually represent the reliability testing goals and outcomes. This makes it easier for engineers and decision-makers to understand the relationship between the test duration, the number of failures, and the reliability target.

The RDC supports decision-making by clearly showing whether the product meets the reliability requirements. If the data points fall within the acceptable area of the chart, the product is considered to have met its reliability goals.

The RDC is valuable both in the planning phase of reliability testing, to determine necessary sample sizes and test durations, and in the evaluation phase, to assess whether the reliability objectives have been achieved.

## Disadvantages of RDC

Complexity in Interpretation: For those unfamiliar with reliability engineering principles, interpreting the RDC can be challenging. It requires a solid understanding of reliability metrics and statistical methods.

Dependence on Accurate Assumptions: The effectiveness of an RDC is heavily dependent on the accuracy of the assumptions made regarding the underlying reliability model and the distribution of failure times. Incorrect assumptions can lead to misleading conclusions.

Sample Size Limitations: The RDC approach may require a relatively large sample size to demonstrate reliability, especially for highly reliable products. This can lead to higher costs and longer testing times.

Does Not Account for All Types of Data: The standard RDC may not be suitable for all types of reliability data, especially in cases where there are complex failure modes or when the data does not follow a common reliability distribution.

Potential for Over-Simplification: There's a risk that the simplicity of the visual representation might lead to over-simplification of complex reliability phenomena, potentially leading stakeholders to overlook important nuances.

In conclusion, while the RDC is a powerful tool for demonstrating and planning for product reliability, its effectiveness hinges on a proper understanding of its principles, careful planning, and accurate assumptions. It should be used as part of a comprehensive reliability engineering strategy that includes a thorough analysis of all relevant factors and potential limitations.

## test data used

According to the assignment requirements, we need to convert our selected input data into a compatible input file. We chose J5.DAT as our test data and convert the data into a xlsx file. 
![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/d1a0de65-25fb-44ed-aa28-6c68d91c4931)

## Result of model comparison (selecting top two models)
![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/80669fcf-3e9a-401e-9009-381ea1040221)
![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/4401ee64-6a61-4063-86b9-8b012444ff83)

To select the top two models from the comparison table, we should consider both the log-likelihood and the Akaike Information Criterion (AIC). The log-likelihood measures how well the model fits the data, with higher values indicating a better fit. However, models with more parameters can overfit the data, which is why we also look at the AIC. The AIC adjusts the log-likelihood to penalize for the number of parameters in the model, thus favoring models that achieve a good fit with fewer parameters. The most suitable model is generally the one with the lowest AIC, as it implies the best balance between goodness of fit and parsimony. Based on these criteria, 'S' and 'DW3' emerge as the top models given that they have the lowest AIC values among the candidates, suggesting they are the most efficient at explaining the data without unnecessary complexity.

## Result of range analysis (an explanation of which part of data is good for proceeding with the analysis)

Upon analysis of the system's failure data collected during integration testing, we observed the failure count over the time intervals. While there is some fluctuation in the data, there isn't a distinct trend of stabilization or consistent decrease in the failure count throughout the intervals. Our evaluation of the failure data, represented over sequential time intervals, indicates a pattern that is approximately linear, without a significant upward or downward trend. Given this observation and the absence of a clear 'burn-in' effect or a stabilization phase that would suggest a mature system, we have determined that all the data should be considered for proceeding with the analysis. This approach allows for a comprehensive assessment of the system's reliability throughout the entire testing period, capturing the full scope of system behavior and failure occurrences.

## Plots for failure rate and reliability of the SUT for the test data provided

### MVF graph

![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/1f944f0e-b834-48fa-aa00-af8f97b2f000)


### Intensity graph

![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/9070346d-3750-4920-9054-154771f1e3cc)

### Reliability graph

![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/33d2d976-e979-48a2-8513-89c06553ae46)

## A discussion on decision making given a target failure rate

In evaluating our system using the reliability assessment tools, we identified that our current failure rate stands at approximately 5 per week, over a span of 73 weeks, resulting in a cumulative count of 367 failures. Our objective was to decrease this rate, and we established a more stringent target failure intensity of 3.00.

With this target in mind, we scrutinized various models to predict the system's reliability and failure intensity over time. Among these, we chose to focus on the DW3 and S models. In our analysis, we aimed to strike a balance between the prediction offered by the DW3 model and the S model, which is why we settled on a median value of 3.00 as our target. This value represents a point between the lowest predictions of the DW3 and S models and is therefore a realistic yet ambitious goal for system reliability.

This decision-making process acknowledges the current state of our system and leverages statistical modeling to set a feasible improvement target. As we progress, we will need to continue assessing the system against this benchmark, adjusting our development and testing efforts accordingly to ensure we meet our reliability objectives. The graph presented in the software tool helps visualize the trajectory towards meeting our target failure intensity and serves as a guide for our ongoing reliability enhancement strategies.

## A discussion on the advantages and disadvantages of reliability growth analysis

### Advantages and Disadvantages of Reliability Growth Analysis

| **Advantages**                                                                 | **Disadvantages**                                                                  |
|-------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| **1. Improved Reliability Over Time:** Helps identify and fix faults, leading to improved system reliability as testing progresses. | **1. Time and Resource Intensive:** Requires substantial time and resources to implement and analyze data effectively.  |
| **2. Predictive Insights:** Allows predictions of future reliability and necessary improvements based on current test data.       | **2. Complex Data Requirements:** Needs detailed and accurate failure data to be effective, which can be challenging to gather. |
| **3. Decision Support:** Supports decision making by providing data-driven insights into when a system may meet reliability targets. | **3. Model Dependency:** The accuracy of the analysis heavily depends on the chosen growth model, which may not always fit the actual system behavior. |
| **4. Systematic Approach:** Provides a structured method to assess and enhance reliability, integrating well with quality assurance processes. | **4. Potentially Misleading:** Without proper understanding and handling, the analysis might lead to incorrect conclusions about the system's reliability. |
| **5. Continuous Improvement:** Encourages ongoing improvements and is iterative, which helps in refining the system progressively.    | **5. Limited by Operational Environment:** Real-world environmental and operational conditions might not be fully replicated in test environments, limiting the applicability of the results. |

## Conclusion

Reliability growth analysis is a powerful tool for improving system reliability through systematic testing and data analysis. However, it requires careful implementation and a deep understanding of the models used to ensure its effectiveness. The technique's benefits must be weighed against its complexity and resource demands.


# Assessment Using Reliability Demonstration Chart 

# 

# Comparison of Results

# Discussion on Similarity and Differences of the Two Techniques

The two failure data assessment techniques we employed, while similar in some respects, exhibit distinct differences when examined in detail:

### Similarities:

- Both methodologies are deployed for assessing a system's reliability.
- Both require comprehensive data to yield precise outcomes.
- They both pinpoint components within the system that fall short in terms of reliability.
- They enable evaluators to ascertain whether the desired reliability threshold has been achieved.

### Differences:

- Reliability growth testing is more apt for predicting potential future performance, whereas the Reliability Demonstration Chart (RDC) is tailored for assessing the current performance level of the software.
- Reliability growth testing adopts a dynamic approach to testing, in contrast to the static nature of RDC.
- The process of reliability growth testing involves ongoing interaction, contrasting with RDC's approach of a singular, conclusive analysis.
- Reliability growth testing excels in identifying system defects, whereas RDC is particularly effective in verifying that software fulfills specific reliability criteria.


# How the team work/effort was divided and managed
In our group, we distributed the workload evenly for lab assignment. Our initial focus was on comprehending the installation and application of the tools, as well as deciphering the data. Subsequently, we delved into the capabilities of each tool and their significance in system testing. Following the execution of the experiments and the analysis of the outcomes, all group members participated in assembling a detailed lab report with meticulousness and precision.

# Difficulties encountered, challenges overcome, and lessons learned

A variety of tools are available for assessing a system's reliability, but they are merely simulations and should not be a substitute for thorough testing. Their reliability is not assured as much of the data they generate is estimated and approximated. Furthermore, the use of these tools necessitates a thorough understanding of each function and the interpretation of results. However, gaining knowledge about these tools is crucial as they are commonly used in the industry, and familiarity with them can be beneficial in the future. We encountered a challenge when the required tool SRTAT was incompatible with our laptops. Initially, we faced difficulties in identifying the steps to derive the results of the assignment tasks. However, through persistent efforts, we were able to understand the concepts by referring to the lecture notes and conducting supplementary research.

# Comments/feedback on the lab itself

For future enhancements of the laboratory experience, it is recommended to employ software that is universally compatible with all operating systems. Additionally, providing comprehensive instructions for tool usage could significantly streamline the learning process, thereby reducing the time spent on trial-and-error methods, which currently seem inefficient. The time allocated for the lab could also be extended, as the existing time constraints necessitate prioritization and limit the opportunity for students to optimize their results.
