**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 – Software Reliability Assessment**

| Group \#:       |   |
|-----------------|---|
| Student 1  |  Zhifan Li |
|  Student 2| Sandip Mishra  |
| Student 3  |  Shanzi Ye |
|   Student 4 |  Fardin Aryan |

# Introduction


This assignment introduced us to the process of using reliability assessment tools to evaluate integration test results, with a focus on the significance of maintaining system quality and dependability. We used two techniques for failure data assessment:

1. Reliability growth testing
2. Reliability assessment via the Reliability Demonstration Chart (RDC)

These techniques are crucial for real-world applications because they offer insightful information about the dependability and efficacy of systems. This makes it possible for programmers to identify such problems early and take the necessary corrective action. By using these evaluation techniques, stakeholders may make well-informed judgments on the dependability of their systems and put policies in place to improve their lifespan and performance.

This lab's goal was to provide participants hands-on experience evaluating a hypothetical system's reliability by examining failure data gathered via integration testing. In order to accomplish this, the lab was split into two portions, each of which concentrated on a failure data assessment technique listed above.

We had to set up a reliability growth assessment tool (like SRTAT or CSFRAT) and create diagrams showing the failure rate and reliability of the system under test (SUT) in the first lab segment. In order to become acquainted with the functions and capabilities of a reliability growth testing tool, we decided to conduct our reliability growth testing using C-SRTAT.

The Reliability Demonstration Chart (RDC) tool and its use were covered in the second section. Understanding how to plot the test data to determine the appropriateness of testing for a specific Mean Time To Failure (MTTF) of the SUT was one of the main objectives of this part.

We now have a better knowledge of dependability growth testing and its significance after completing this lab. The testing instruments utilized in the experiment, such as C-SFRAT or SRTAT, are described in this lab report. These tools were applied to the failure data set, which consists of a variety of test data files. For every failure data analysis approach, we chose one file.

# Assessment Using Reliability Growth Testing

## test data used

According to the assignment requirements, we need to convert our selected input data into a compatible input file. We chose J5.DAT as our test data and convert the data into a xlsx file. 
![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/d1a0de65-25fb-44ed-aa28-6c68d91c4931)

## Result of model comparison (selecting top two models)
![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/80669fcf-3e9a-401e-9009-381ea1040221)
![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/4401ee64-6a61-4063-86b9-8b012444ff83)

We should take into account both the log-likelihood and the Akaike Information Criterion (AIC) when choosing the top two models from the comparison table. Higher values in the log-likelihood indicate a better fit between the model and the data. We additionally consider the AIC since models with more parameters have the potential to overfit the data. Models that obtain a good fit with fewer parameters are favored by the AIC, which modifies the log-likelihood to penalize for the number of parameters in the model.  The model with the lowest AIC is usually the most appropriate since it suggests the optimal trade-off between parsimony and goodness of fit. 'S' and 'DW3' are the best models according to these criteria since they have the lowest AIC values among the contenders, indicating that they are the most effective in explaining the data in an unnecessarily complex way.

## Result of range analysis (an explanation of which part of data is good for proceeding with the analysis)

We observed the failure count over the time intervals after analyzing the system's failure data that was gathered during integration testing. Although the data fluctuates a little, there isn't a clear trend of stabilization or a steady decline in the number of failures throughout the intervals. Our analysis of the failure data, which is displayed over several time intervals, shows a roughly linear pattern devoid of any discernible upward or decreasing trend. Proceeding with the observation, we have concluded that all the data should be taken into consideration given this observation and the lack of a distinct 'burn-in' impact or stabilizing phase that would indicate a mature system. This method captures the whole range of system behavior and failure events and enables a thorough evaluation of the system's reliability during the course of the testing phase.

## Plots for failure rate and reliability of the SUT for the test data provided

### MVF graph

![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/1f944f0e-b834-48fa-aa00-af8f97b2f000)


### Intensity graph

![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/9070346d-3750-4920-9054-154771f1e3cc)

### Reliability graph

![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/aa908d3c-0e9b-4aba-9758-3dddc1d55ef7)

## A discussion on decision making given a target failure rate

After utilizing the reliability evaluation tools to evaluate our system, we found that, after a period of 73 weeks, the total count of 367 failures is approximately 5 failures per week. Reducing this rate was our goal, and we set a more demanding target failure intensity of 3.00.

In order to estimate the system's dependability and failure severity over time, we carefully examined a number of models. We decided to concentrate on the DW3 and S models among them. In our analysis, we aimed to strike a balance between the prediction offered by the DW3 model and the S model, which is why we settled on a median value of 3.00 as our target. This value represents a point between the lowest predictions of the DW3 and S models and is therefore a realistic yet ambitious goal for system reliability.

This decision-making process acknowledges the current state of our system and leverages statistical modeling to set a feasible improvement target. As we progress, we will need to continue assessing the system against this benchmark, adjusting our development and testing efforts accordingly to ensure we meet our reliability objectives. The graph presented in the software tool helps visualize the trajectory towards meeting our target failure intensity and serves as a guide for our ongoing reliability enhancement strategies.

## A discussion on the advantages and disadvantages of reliability growth analysis

### Advantages and Disadvantages of Reliability Growth Analysis

| **Advantages**                                                                 | **Disadvantages**                                                                  |
|-------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| **1. Improved Reliability Over Time:** Helps identify and fix faults, leading to improved system reliability as testing progresses. | **1. Time and Resource Intensive:** Requires substantial time and resources to implement and analyze data effectively.  |
| **2. Predictive Insights:** Allows predictions of future reliability and necessary improvements based on current test data.       | **2. Complex Data Requirements:** Needs detailed and accurate failure data to be effective, which can be challenging to gather. |
| **3. Decision Support:** Supports decision making by providing data-driven insights into when a system may meet reliability targets. | **3. Model Dependency:** The accuracy of the analysis heavily depends on the chosen growth model, which may not always fit the actual system behavior. |
| **4. Systematic Approach:** Provides a structured method to assess and enhance reliability, integrating well with quality assurance processes. | **4. Potentially Misleading:** Without proper understanding and handling, the analysis might lead to incorrect conclusions about the system's reliability. |
| **5. Continuous Improvement:** Encourages ongoing improvements and is iterative, which helps in refining the system progressively.    | **5. Limited by Operational Environment:** Real-world environmental and operational conditions might not be fully replicated in test environments, limiting the applicability of the results. |

## Conclusion

Reliability growth analysis is a powerful tool for improving system reliability through systematic testing and data analysis. However, it requires careful implementation and a deep understanding of the models used to ensure its effectiveness. The technique's benefits must be weighed against its complexity and resource demands.

# Assessment Using Reliability Demonstration Chart

## 3 Plots for MTTFMin, Twice and Half of It for Your Test Data
![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/64334331/91e616c1-794b-4119-b16e-958b0a2e76d4)
![method]([https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/64334331/91e616c1-794b-4119-b16e-958b0a2e76d4]

In the data drawn from Failure Report 8, we filled out sections of the Reliability-Demonstration-Chart spreadsheet to perform RDC testing. This dataset records 16 defects emerging over several days. We're adjusting the maximum threshold of tolerable defects per quantity of input events to scrutinize its impact on the Mean Time To Failure (MTTF).

<img width="468" alt="image" src="https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/64334331/0ca95513-cadb-4582-acd2-e82676a9d163">

This analysis reveals a Fault Incidence Rate (FIR) of 4 defects every 43,200 seconds, equating to 4 defects per 12-hour period.

It is noted that the MTTF, given a defect rate of 4 per 12 hours, is calculated to be 10,800 seconds, or 3 hours. Under these conditions, the graph remains in the “continue testing” zone until the occurrence of the 9th event, at which point it transitions into the “accept” category. Thus, the minimum MTTF is established at 3 hours.

<img width="468" alt="image" src="https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/64334331/78ba234c-aa13-4c5a-b9c3-ba3e1883bf59">

Another observation indicates a FIR of 4 defects over 86,400 seconds, which corresponds to 4 defects every 24 hours.

From this, it is evident that the MTTF at a defect rate of 4 per 24 hours reaches 21,600 seconds, or 6 hours. Given these metrics, the graph initially falls within the “continue testing” area but eventually moves into the “reject” territory. The MTTF of 6 hours represents twice the minimum MTTF. Increasing the MTTF heightens the minimum acceptance criteria, causing the graph to shift toward the “reject” region.

The final assessment shows a FIR of 4 defects every 21,600 seconds, translating to 4 defects every 6 hours.

<img width="468" alt="image" src="https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/64334331/4824ab7c-b7ff-4640-b733-0b0226589a0a">

This finding shows that the MTTF, at a defect frequency of 4 per 6 hours, stands at 5,400 seconds, or 1.5 hours. Under these parameters, the chart begins in the “continue testing” area before rapidly entering the “accept” territory. The MTTF of 1.5 hours is half of the minimum MTTF. Lowering the MTTF reduces the minimum acceptance criteria, moving the chart into the “accept” region.

## Explain Your Evaluation and Justification of How You Decide The MTTFmin

The minimum Mean Time to Failure (MTTFmin) is based on a system's capability of handling certain failures, stress. It demonstrates the reliability of the system. After comparing and analyzing these three graphs, we made a conclusion that MTTFmin is ~2.9 hours. Which is the time when the line starts entering the accepting zone.

## Advantages of RDC

One of the primary advantages of the RDC is its ability to visually represent the reliability testing goals and outcomes. This makes it easier for engineers and decision-makers to understand the relationship between the test duration, the number of failures, and the reliability target.

The RDC supports decision-making by clearly showing whether the product meets the reliability requirements. If the data points fall within the acceptable area of the chart, the product is considered to have met its reliability goals.

The RDC is valuable both in the planning phase of reliability testing, to determine necessary sample sizes and test durations, and in the evaluation phase, to assess whether the reliability objectives have been achieved.

## Disadvantages of RDC

Complexity in Interpretation: For those unfamiliar with reliability engineering principles, interpreting the RDC can be challenging. It requires a solid understanding of reliability metrics and statistical methods.

Dependence on Accurate Assumptions: The effectiveness of an RDC is heavily dependent on the accuracy of the assumptions made regarding the underlying reliability model and the distribution of failure times. Incorrect assumptions can lead to misleading conclusions.

Sample Size Limitations: The RDC approach may require a relatively large sample size to demonstrate reliability, especially for highly reliable products. This can lead to higher costs and longer testing times.

Does Not Account for All Types of Data: The standard RDC may not be suitable for all types of reliability data, especially in cases where there are complex failure modes or when the data does not follow a common reliability distribution.

Potential for Over-Simplification: There's a risk that the simplicity of the visual representation might lead to over-simplification of complex reliability phenomena, potentially leading stakeholders to overlook important nuances.

In conclusion, while the RDC is a powerful tool for demonstrating and planning for product reliability, its effectiveness hinges on a proper understanding of its principles, careful planning, and accurate assumptions. It should be used as part of a comprehensive reliability engineering strategy that includes a thorough analysis of all relevant factors and potential limitations.

# Comparison of Results

### Reliability Growth Testing Results Analysis (RGT): 
- Used the complete data set for a good representation of system performance.
- Used to enhance system reliability over time through iterative failure analysis
- The appropriateness of these models was validated through graphical analysis. 
- Needs detailed and accurate failure data to be effective, which can be hard to get.
- Allows for continuous improvements, and helps in refining the system.
- Accuracy of the analysis depends on the chosen growth model, which may not always fit the system behavior.
- More data the better for this model for better results

### Reliability Demonstration Chart  Results Analysis (RDC): 
- Visual representation for reliability testing goals and outcomes.
- Shows  relationship between test duration, number of failures, and reliability target. 
- Supports decision-making by indicating if the system meets reliability requirements. 
- Data points in the range (acceptable) area of the chart show meeting reliability goals.
- Checks if reliability reqs are met at a specific snapshot in time
- Snapshot of failure data to check against reliability requirements
- More data the better for this model for better results


# Discussion on Similarity and Differences of the Two Techniques

The two failure data assessment techniques we employed, while similar in some respects, exhibit distinct differences when examined in detail:

### Similarities:

- Both methodologies are deployed for assessing a system's reliability.
- Both require comprehensive data to yield precise outcomes.
- They both pinpoint components within the system that fall short in terms of reliability.
- They enable evaluators to ascertain whether the desired reliability threshold has been achieved.

### Differences:

- Reliability growth testing is more apt for predicting potential future performance, whereas the Reliability Demonstration Chart (RDC) is tailored for assessing the current performance level of the software.
- Reliability growth testing adopts a dynamic approach to testing, in contrast to the static nature of RDC.
- The process of reliability growth testing involves ongoing interaction, contrasting with RDC's approach of a singular, conclusive analysis.
- Reliability growth testing excels in identifying system defects, whereas RDC is particularly effective in verifying that software fulfills specific reliability criteria.


# How the team work/effort was divided and managed
In our group, we distributed the workload evenly for lab assignment. Our initial focus was on comprehending the installation and application of the tools, as well as deciphering the data. Subsequently, we delved into the capabilities of each tool and their significance in system testing. Following the execution of the experiments and the analysis of the outcomes, all group members participated in assembling a detailed lab report with meticulousness and precision.

# Difficulties encountered, challenges overcome, and lessons learned

A variety of tools are available for assessing a system's reliability, but they are merely simulations and should not be a substitute for thorough testing. Their reliability is not assured as much of the data they generate is estimated and approximated. Furthermore, the use of these tools necessitates a thorough understanding of each function and the interpretation of results. However, gaining knowledge about these tools is crucial as they are commonly used in the industry, and familiarity with them can be beneficial in the future. We encountered a challenge when the required tool SRTAT was incompatible with our laptops. Initially, we faced difficulties in identifying the steps to derive the results of the assignment tasks. However, through persistent efforts, we were able to understand the concepts by referring to the lecture notes and conducting supplementary research.

# Comments/feedback on the lab itself

For future enhancements of the laboratory experience, it is recommended to employ software that is universally compatible with all operating systems. Additionally, providing comprehensive instructions for tool usage could significantly streamline the learning process, thereby reducing the time spent on trial-and-error methods, which currently seem inefficient. The time allocated for the lab could also be extended, as the existing time constraints necessitate prioritization and limit the opportunity for students to optimize their results.
