**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 â€“ Software Reliability Assessment**

| Group \#:       |   |
|-----------------|---|
| Student 1  |  Zhifan Li |
|  Student 2| Sandip Mishra  |
| Student 3  |  Shanzi Ye |
|   Student 4 |  Fardin Aryan |

# Introduction



This assignment provided us with an introduction to the evaluation of integration test data through the use of reliability assessment tools, emphasizing the importance of ensuring system quality and reliability. We implemented two failure data assessment methods:

1. Reliability growth testing
2. Reliability assessment via the Reliability Demonstration Chart (RDC)

These methods are of paramount importance in practical applications as they provide valuable insights into system effectiveness and reliability. This allows for early detection of potential issues by programmers, who can then take appropriate corrective measures. The use of these assessment tools enables stakeholders to make informed decisions about their systems' reliability and implement measures to enhance their performance and lifespan.

The objective of this lab was to gain practical experience in assessing the reliability of a hypothetical system by analyzing its failure data collected during integration testing. To achieve this, the lab was divided into two sections, each focusing on one of the above-mentioned failure data assessment methods.

In the first section of the lab, we were required to establish a reliability growth assessment tool, such as SRTAT or CSFRAT, and generate diagrams depicting the failure rate and reliability of the system under test (SUT). We chose to use SRTAT for our reliability growth testing, gaining familiarity with the features and operation of a reliability growth testing tool.

In the second section, we were introduced to the Reliability Demonstration Chart (RDC) tool and its application. A key goal of this section was to understand how to ascertain the adequacy of testing for a given Mean Time To Failure (MTTF) of the SUT by plotting the test data.

Upon completion of this lab, we have gained a deeper understanding of reliability growth testing and its importance. This lab report details the testing tools used in the experiment, including C-SFRAT or SRTAT, applied to the provided failure data set, which includes various test data files. We selected one file for each of the failure data analysis methods.

# Assessment Using Reliability Growth Testing 

## test data used

According to the assignment requirements, we need to convert our selected input data into a compatible input file. We chose J5.DAT as our test data and convert the data into a xlsx file. 
![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/d1a0de65-25fb-44ed-aa28-6c68d91c4931)

## Result of model comparison (selecting top two models)
![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/80669fcf-3e9a-401e-9009-381ea1040221)
![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/4401ee64-6a61-4063-86b9-8b012444ff83)

To select the top two models from the comparison table, we should consider both the log-likelihood and the Akaike Information Criterion (AIC). The log-likelihood measures how well the model fits the data, with higher values indicating a better fit. However, models with more parameters can overfit the data, which is why we also look at the AIC. The AIC adjusts the log-likelihood to penalize for the number of parameters in the model, thus favoring models that achieve a good fit with fewer parameters. The most suitable model is generally the one with the lowest AIC, as it implies the best balance between goodness of fit and parsimony. Based on these criteria, 'S' and 'DW3' emerge as the top models given that they have the lowest AIC values among the candidates, suggesting they are the most efficient at explaining the data without unnecessary complexity.

## Result of range analysis (an explanation of which part of data is good for proceeding with the analysis)

## Plots for failure rate and reliability of the SUT for the test data provided

### MVF graph

![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/1f944f0e-b834-48fa-aa00-af8f97b2f000)


### Intensity graph

![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/9070346d-3750-4920-9054-154771f1e3cc)

### Reliability graph

![method](https://github.com/seng438-winter-2024/seng438-a5-zhifanl/assets/110203582/33d2d976-e979-48a2-8513-89c06553ae46)

## A discussion on decision making given a target failure rate

## A discussion on the advantages and disadvantages of reliability growth analysis

# Advantages and Disadvantages of Reliability Growth Analysis

## | **Advantages**                                                                 | **Disadvantages**                                                                  |
|-------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| **1. Improved Reliability Over Time:** Helps identify and fix faults, leading to improved system reliability as testing progresses. | **1. Time and Resource Intensive:** Requires substantial time and resources to implement and analyze data effectively.  |
| **2. Predictive Insights:** Allows predictions of future reliability and necessary improvements based on current test data.       | **2. Complex Data Requirements:** Needs detailed and accurate failure data to be effective, which can be challenging to gather. |
| **3. Decision Support:** Supports decision making by providing data-driven insights into when a system may meet reliability targets. | **3. Model Dependency:** The accuracy of the analysis heavily depends on the chosen growth model, which may not always fit the actual system behavior. |
| **4. Systematic Approach:** Provides a structured method to assess and enhance reliability, integrating well with quality assurance processes. | **4. Potentially Misleading:** Without proper understanding and handling, the analysis might lead to incorrect conclusions about the system's reliability. |
| **5. Continuous Improvement:** Encourages ongoing improvements and is iterative, which helps in refining the system progressively.    | **5. Limited by Operational Environment:** Real-world environmental and operational conditions might not be fully replicated in test environments, limiting the applicability of the results. |

## Conclusion

Reliability growth analysis is a powerful tool for improving system reliability through systematic testing and data analysis. However, it requires careful implementation and a deep understanding of the models used to ensure its effectiveness. The technique's benefits must be weighed against its complexity and resource demands.


# Assessment Using Reliability Demonstration Chart 

# 

# Comparison of Results

# Discussion on Similarity and Differences of the Two Techniques

The two failure data assessment techniques we employed, while similar in some respects, exhibit distinct differences when examined in detail:

### Similarities:

- Both methodologies are deployed for assessing a system's reliability.
- Both require comprehensive data to yield precise outcomes.
- They both pinpoint components within the system that fall short in terms of reliability.
- They enable evaluators to ascertain whether the desired reliability threshold has been achieved.

### Differences:

- Reliability growth testing is more apt for predicting potential future performance, whereas the Reliability Demonstration Chart (RDC) is tailored for assessing the current performance level of the software.
- Reliability growth testing adopts a dynamic approach to testing, in contrast to the static nature of RDC.
- The process of reliability growth testing involves ongoing interaction, contrasting with RDC's approach of a singular, conclusive analysis.
- Reliability growth testing excels in identifying system defects, whereas RDC is particularly effective in verifying that software fulfills specific reliability criteria.


# How the team work/effort was divided and managed
In our group, we distributed the workload evenly for lab assignment. Our initial focus was on comprehending the installation and application of the tools, as well as deciphering the data. Subsequently, we delved into the capabilities of each tool and their significance in system testing. Following the execution of the experiments and the analysis of the outcomes, all group members participated in assembling a detailed lab report with meticulousness and precision.

# Difficulties encountered, challenges overcome, and lessons learned

A variety of tools are available for assessing a system's reliability, but they are merely simulations and should not be a substitute for thorough testing. Their reliability is not assured as much of the data they generate is estimated and approximated. Furthermore, the use of these tools necessitates a thorough understanding of each function and the interpretation of results. However, gaining knowledge about these tools is crucial as they are commonly used in the industry, and familiarity with them can be beneficial in the future. We encountered a challenge when the required tool SRTAT was incompatible with our laptops. Initially, we faced difficulties in identifying the steps to derive the results of the assignment tasks. However, through persistent efforts, we were able to understand the concepts by referring to the lecture notes and conducting supplementary research.

# Comments/feedback on the lab itself

For future enhancements of the laboratory experience, it is recommended to employ software that is universally compatible with all operating systems. Additionally, providing comprehensive instructions for tool usage could significantly streamline the learning process, thereby reducing the time spent on trial-and-error methods, which currently seem inefficient. The time allocated for the lab could also be extended, as the existing time constraints necessitate prioritization and limit the opportunity for students to optimize their results.
